import numpy as np
import matplotlib.pyplot as plt

x = np.outer(np.linspace(-1, 1, 100), np.ones(100))                                             # Creating dataset
y = x.copy().T 
z = x ** 2 + y ** 2

fig = plt.figure(figsize=(14, 9))
ax = plt.axes(projection='3d')
ax.plot_surface(x, y, z, cmap='viridis')

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
ax.set_title('Surface Plot')

plt.show()


---------------------------



from queue import PriorityQueue
v=14
graph=[[] for i in range(v)]

def bfs(src,dest,n):
    visited=[False]*n
    pq=PriorityQueue()
    pq.put((0,src))
    visited[src]=True
    
    while pq.empty()==False:
        u=pq.get()[1]
        print(u,end=" ")
        if u==dest:
            break
            
        for v,c in graph[u]:
            if visited[v]==False:
                visited[v]=True
                pq.put((c,v))
    print()
    
def addedge(x,y,cost):
    graph[x].append((y,cost))
    graph[y].append((x,cost))
    
addedge(0, 1, 3)
addedge(0, 2, 6)
addedge(0, 3, 5)
addedge(1, 4, 9)
addedge(1, 5, 8)
addedge(2, 6, 12)
addedge(2, 7, 14)
addedge(3, 8, 7)
addedge(8, 9, 5)
addedge(8, 10, 6)
addedge(9, 11, 1)
addedge(9, 12, 10)
addedge(9, 13, 2)


src=0
dest=9
bfs(src,dest,v)




------------------------------------------------




import numpy as np
import matplotlib.pyplot as plt

x=np.linspace(-5,5,100)
y=np.linspace(-5,5,100)
X,Y=np.meshgrid(x,y)
Z=np.cos(2*X)*np.sin(2*Y)

fig,ax=plt.subplots(figsize=(8,6))
contour=ax.contour(X,Y,Z,levels=15,cmap='coolwarm')

ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('contour_plot')

plt.show()

------------------------------------------------



def aStarAlgo(start_node, stop_node):
    open_set = set(start_node)
    closed_set = set()
    g = {}             
    parents = {}         
    g[start_node] = 0
    parents[start_node] = start_node

    while len(open_set) > 0:
        n = None
        for v in open_set:
            if n == None or g[v] + heuristic(v) < g[n] + heuristic(n):
                n = v
        if n == stop_node or Graph_nodes[n] == None:
            pass
        else:
            for (m, weight) in get_neighbors(n):
                if m not in open_set and m not in closed_set:
                    open_set.add(m)
                    parents[m] = n
                    g[m] = g[n] + weight

                else:
                    if g[m] > g[n] + weight:
                        g[m] = g[n] + weight
                        parents[m] = n
                        
                        if m in closed_set:
                            closed_set.remove(m)
                            open_set.add(m)
        if n == None:
            print('Path does not exist!')
            return None
        
        if n == stop_node:
            path = []
            while parents[n] != n:
                path.append(n)
                n = parents[n]
            path.append(start_node)
            path.reverse()
            print('Path found: {}'.format(path))
            return path
        open_set.remove(n)
        closed_set.add(n)
    print('Path does not exist!')
    return None

def get_neighbors(v):
    if v in Graph_nodes:
        return Graph_nodes[v]
    else:
        return None

def heuristic(n):
    H_dist = {
        'A': 11,
        'B': 6,
        'C': 5,
        'D': 7,
        'E': 3,
        'F': 6,
        'G': 5,
        'H': 3,
        'I': 1,
        'J': 0
    }
    return H_dist[n]

Graph_nodes = {
    'A': [('B', 6), ('F', 3)],
    'B': [('A', 6), ('C', 3), ('D', 2)],
    'C': [('B', 3), ('D', 1), ('E', 5)],
    'D': [('B', 2), ('C', 1), ('E', 8)],
    'E': [('C', 5), ('D', 8), ('I', 5), ('J', 5)],
    'F': [('A', 3), ('G', 1), ('H', 7)],
    'G': [('F', 1), ('I', 3)],
    'H': [('F', 7), ('I', 2)],
    'I': [('E', 5), ('G', 3), ('H', 2), ('J', 3)],
}

aStarAlgo('A', 'J')


-------------------------------------------------



import numpy as np
import seaborn as sn
import matplotlib.pyplot as plt

data = np.random.randint(low = 1,high = 100,size = (10, 10))                     # generating 2-D 10x10 matrix of random numbers from 1 to 100
print("The data to be plotted:\n")
print(data)

hm = sn.heatmap(data = data)

plt.show()

--------------------------------------------------

import math

def minimax (curDepth, nodeIndex, maxTurn, scores,targetDepth):

    if (curDepth == targetDepth):
        return scores[nodeIndex]

    if (maxTurn):
        return max(minimax(curDepth + 1, nodeIndex * 2,False, scores, targetDepth),minimax(curDepth + 1, nodeIndex * 2 + 1,False, scores, targetDepth))

    else:
        return min(minimax(curDepth + 1, nodeIndex * 2,True, scores, targetDepth),minimax(curDepth + 1, nodeIndex * 2 + 1,True, scores, targetDepth))

scores = [3, 5, 2, 9, 12, 5, 23, 23]
treeDepth = math.log(len(scores), 2)

print("The optimal value is : ", end = "")
print(minimax(0, 0, True, scores, treeDepth))


----------------------------------------------------


import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris

iris = load_iris()
data = iris.data
target = iris.target
feature_names = iris.feature_names

plt.figure(figsize=(8, 6))
sns.boxplot(data=data)
plt.xticks(ticks=range(len(feature_names)), labels=feature_names, rotation=45)

plt.xlabel('Features')
plt.ylabel('Values')
plt.title('Box Plot of Iris Dataset')
plt.show()

------------------------------------------------------


MAX, MIN = 1000, -1000

def minimax(depth, nodeIndex, maximizingPlayer,values, alpha, beta):
    if depth == 3:
        return values[nodeIndex]
 
    if maximizingPlayer:
        best = MIN
        for i in range(0, 2):
            val = minimax(depth + 1, nodeIndex * 2 + i, False, values, alpha, beta)
            best = max(best, val)
            alpha = max(alpha, best)
            if beta <= alpha:
                break
          
        return best
      
    else:
        best = MAX
        for i in range(0, 2):
            val = minimax(depth + 1, nodeIndex * 2 + i,True, values, alpha, beta)
            best = min(best, val)
            beta = min(beta, best)
            if beta <= alpha:
                break
          
        return best

values = [3, 5, 6, 9, 1, 2, 0, -1] 
print("The optimal value is :", minimax(0, 0, True, values, MIN, MAX))


---------------------------------------------------------


import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from collections import Counter
from sklearn.preprocessing import LabelEncoder

def manhattan(x1,x2):
    return np.sum(np.abs(x1-x2))

class KNN:
    def __init__(self,k=3):
        self.k=k
        
    def fit(self,X,y):
        self.X_train=X
        self.y_train=y
        
    def predict(self,X):
        return [self._prediction(x) for x in X]
    
    def _prediction(self,x):
        distance=[manhattan(x,x_train) for x_train in self.X_train]
        k_indices=np.argsort(distance)[:self.k]
        k_nearest_neighbor=[self.y_train[i] for i in k_indices]
        most_common=Counter(k_nearest_neighbor).most_common()
        return most_common[0][0]
    
df=pd.read_csv('fruit.csv')
label_encoder=LabelEncoder()
df['fruit_label']=label_encoder.fit_transform(df['fruit_label'])
df['mass']=df['mass'].factorize()[0]


X=df.drop('fruit_label',axis=1).values
y=df['fruit_label'].values
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1234)

clf=KNN(k=5)
clf.fit(X_train,y_train)
prediction=clf.predict(X_test)
print(prediction)

accuracy=np.sum(prediction==y_test)/len(y_test)
print(accuracy)

# output
# [0, 3, 0, 0, 1, 3, 2, 2, 0, 2, 3, 2, 1, 0, 3, 2, 0, 3]
# 0.8888888888888888

----------------------------------------------

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

def kmeans(X, K, max_iters=100):
    centroids = X[:K]

    for _ in range(max_iters):

        labels = np.argmin(np.linalg.norm(X[:, np.newaxis] - centroids, axis=2), axis=1)
        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K)])

        if np.all(centroids == new_centroids):
            break

        centroids = new_centroids

    return labels, centroids

iris = load_iris()
X = iris.data
K = 3
labels, centroids = kmeans(X, K)
print("Labels:", labels)
print("Centroids:", centroids)

plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', color='red', s=200)
plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')
plt.title('K-means Clustering of Iris Dataset')
plt.show()

-----------------------------------------------



from scipy.cluster.hierarchy import dendrogram,linkage
from sklearn.datasets import load_iris
import numpy as np
import matplotlib.pyplot as plt

iris=load_iris()
data=iris.data[:5]
print(data)

def single_linkage(data):
    n=data.shape[0]
    proximity_matrix=np.zeros((n,n))
    
    for i in range(n):
        for j in range(i+1,n):
            proximity_matrix[i,j]=np.min(np.linalg.norm(data[i]-data[j]))
            proximity_matrix[j,i]=proximity_matrix[i,j]
    return proximity_matrix

print(single_linkage(data))

linkageX=linkage(data,method='single')
plt.figure(figsize=(10,6))
dendrogram(linkageX)
plt.xlabel('x')
plt.ylabel('y')
plt.title('single linkage')
plt.show()

def complete_linkage(data):
    n=data.shape[0]
    proximity_matrix=np.zeros((n,n))
    
    for i in range(n):
        for j in range(i+1,n):
            proximity_matrix[i,j]=np.max(np.linalg.norm(data[i]-data[j]))
            proximity_matrix[j,i]=proximity_matrix[i,j]
    return proximity_matrix

print(single_linkage(data))

linkageX=linkage(data,method='complete')
plt.figure(figsize=(10,6))
dendrogram(linkageX)
plt.xlabel('x')
plt.ylabel('y')
plt.title('complete linkage')
plt.show()


--------------------------------------------------


import numpy as np

class PCA:
    def __init__(self,n_components):
        self.n_components=n_components
        self.components=None
        self.mean=None
        
    def fit(self,X):
        self.mean = np.mean(X, axis=0)
        X = X -  self.mean
        cov=np.cov(X.T)
        eigenvectors,eigenvalues=np.linalg.eig(cov)
        
        eigenvectors=eigenvectors.T
        idxs=np.argsort(eigenvalues)[::-1]
        eigenvalues=eigenvalues[idxs]
        eigenvectors=eigenvectors[idxs]
        
        self.components=eigenvectors[:self.n_components]
        
    def transform(self,X):
        X = X - self.mean
        return np.dot(X,self.components.T)
    
if __name__=="__main__":
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_iris
    
    iris=load_iris()
    X=iris.data
    y=iris.target
    
    pca=PCA(2)
    pca.fit(X)
    x_pro=pca.transform(X)
    print(X.shape)
    print(x_pro.shape)
    
    x1=x_pro[:,0]
    x2=x_pro[:,1]
    plt.scatter(x1,x2,c=y,edgecolor="none",alpha=0.8,cmap="viridis")
    plt.xlabel("x axis")
    plt.ylabel("y axis")
    plt.colorbar()
    plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

class LDA:
    def __init__(self, n_components=None):
        self.n_components = n_components
        self.eig_vectors = None
    
    def transform(self,X,y):
        height, width = X.shape
        unique_classes = np.unique(y)
        num_classes = len(unique_classes)

        scatter_t = np.cov(X.T)*(height - 1)
        scatter_w = 0
        for i in range(num_classes):
            class_items = np.flatnonzero(y == unique_classes[i])
            scatter_w = scatter_w + np.cov(X[class_items].T) * (len(class_items)-1)
        
        scatter_b = scatter_t - scatter_w
        _, eig_vectors = np.linalg.eigh(np.linalg.pinv(scatter_w).dot(scatter_b))
        print(eig_vectors.shape)
        pc = X.dot(eig_vectors[:,::-1][:,:self.n_components])
        print(pc.shape)

        if self.n_components == 2:
            if y is None:
                plt.scatter(pc[:,0],pc[:,1])
            else:
                colors = ['r','g','b']
                labels = np.unique(y)
                for color, label in zip(colors, labels):
                    class_data = pc[np.flatnonzero(y==label)]
                    plt.scatter(class_data[:,0],class_data[:,1],c=color)
            plt.show()
        return pc

LDA_obj = LDA(n_components=2)
data = load_iris()
X, y = data.data, data.target
X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)

LDA_object = LDA(n_components=2)
X_train_modified = LDA_object.transform(X_train, Y_train)

print("Original Data Size:",X_train.shape, "\nModified Data Size:", X_train_modified.shape)